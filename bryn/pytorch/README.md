# 4 Layer Neural Network
	- Our NN will be fully connected and will analyze the MNIST dataset.
	- The input layer will have 28 x 28 (= 784) greyscale pixels which make up the MNIST dataset

		* ReLu (Rectified Linear Activated Function): A linear function that will output the input directly if it is a positive

	-  Onece the data has been received at the input layer, it will propogate through two hidden layers, each having 200 nodes