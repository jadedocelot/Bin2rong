{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.18.5)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.3.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autograd\n",
      "  Using cached https://files.pythonhosted.org/packages/23/12/b58522dc2cbbd7ab939c7b8e5542c441c9a06a8eccb00b3ecac04a739896/autograd-1.3.tar.gz\n",
      "Requirement already satisfied: numpy>=1.12 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from autograd) (1.18.5)\n",
      "Requirement already satisfied: future>=0.15.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from autograd) (0.18.2)\n",
      "Building wheels for collected packages: autograd\n",
      "  Building wheel for autograd (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for autograd: filename=autograd-1.3-cp38-none-any.whl size=47990 sha256=caec286ea2b39b880491d46c429f4b0267e25f50663e27651df0332012c09a85\n",
      "  Stored in directory: /Users/edgar/Library/Caches/pip/wheels/42/62/66/1121afe23ff96af4e452e0d15e68761e3f605952ee075ca99f\n",
      "Successfully built autograd\n",
      "Installing collected packages: autograd\n",
      "Successfully installed autograd-1.3\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.3.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/AutomaticDifferentiationNutshell.png/1920px-AutomaticDifferentiationNutshell.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Autograd\n"
     ]
    }
   ],
   "source": [
    "# Tester\n",
    "print(\"Hello Autograd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taylor_sine(x):\n",
    "    ans = currterm = x\n",
    "    i = 0\n",
    "    while np.abs(currterm) > 0.001:\n",
    "        currterm = -currterm * x**2 /((2 * i + 3) * (2 * i + 2))\n",
    "        ans = ans + currterm\n",
    "        i += 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of sin(pie) is  -0.9998995297042174\n"
     ]
    }
   ],
   "source": [
    "grad_sine = grad(taylor_sine)\n",
    "print(\"Gradient of sin(pie) is \",grad_sine(np.pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> A common use case for automatic differentiation is to train a probabilistic model. Here we have a very simple (but complete) example of specifying and training a logistic regression model for binary classification</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 0.5 * (np.tanh(x/2.)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs probability of a label being true according to logistic model.\n",
    "def logistic_predictions(weight,inputs):\n",
    "    return sigmoid (np.dot(inputs,weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss is the negative log-likelihood of the training labels\n",
    "def training_loss(weights):\n",
    "    preds = logistic_predictions(weights,inputs)\n",
    "    label_probabilities = preds * targets + (1 - preds) * (1 - targets)\n",
    "    return -np.sum(np.log(label_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a toy dataset\n",
    "inputs = np.array([[0.52,1.12,0.77],\n",
    "                  [0.88, -1.08,0.15],\n",
    "                  [0.52,0.06,-1.30],\n",
    "                  [0.74,-2.49,1.39]])\n",
    "targets = np.array([True,True,False,True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that returns gradients of training loss using Autograd\n",
    "training_gradient_fun = grad(training_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss:  2.772588722239781\n"
     ]
    }
   ],
   "source": [
    "# Optimize weights using gradient descent\n",
    "weights = np.array([0.0,0.0,0.0])\n",
    "print(\"Initial loss: \",training_loss(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained loss:  2.772588722239781\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    weights -= training_gradient_fun(weights) * 0.01\n",
    "\n",
    "print(\"Trained loss: \",training_loss(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bit13013e1e820345ee95563c74cef2b644"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
