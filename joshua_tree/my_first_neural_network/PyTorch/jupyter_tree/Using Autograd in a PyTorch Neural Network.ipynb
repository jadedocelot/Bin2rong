{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/ort/opt/anaconda3/lib/python3.7/site-packages (1.19.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autograd in /Users/ort/opt/anaconda3/lib/python3.7/site-packages (1.3)\n",
      "Requirement already satisfied: future>=0.15.2 in /Users/ort/opt/anaconda3/lib/python3.7/site-packages (from autograd) (0.18.2)\n",
      "Requirement already satisfied: numpy>=1.12 in /Users/ort/opt/anaconda3/lib/python3.7/site-packages (from autograd) (1.19.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/AutomaticDifferentiationNutshell.png/1920px-AutomaticDifferentiationNutshell.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In mathematics and computer algebra, automatic differentiation (AD), also called algorithmic differentiation, computational differentiation,[1][2] auto-differentiation, or simply autodiff, is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Autograd\n"
     ]
    }
   ],
   "source": [
    "# Tester\n",
    "print(\"Hello Autograd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taylor_sine(x):\n",
    "    ans = currterm = x\n",
    "    i = 0\n",
    "    while np.abs(currterm) > 0.001:\n",
    "        currterm = -currterm * x**2 /((2 * i + 3) * (2 * i + 2))\n",
    "        ans = ans + currterm\n",
    "        i += 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of sin(pie) is  -0.9998995297042174\n"
     ]
    }
   ],
   "source": [
    "grad_sine = grad(taylor_sine)\n",
    "print(\"Gradient of sin(pie) is \",grad_sine(np.pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> A common use case for automatic differentiation is to train a probabilistic model. Here we have a very simple (but complete) example of specifying and training a logistic regression model for binary classification</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 0.5 * (np.tanh(x/2.)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs probability of a label being true according to logistic model.\n",
    "def logistic_predictions(weight,inputs):\n",
    "    return sigmoid (np.dot(inputs,weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loss is the negative log-likelihood of the training labels\n",
    "def training_loss(weights):\n",
    "    preds = logistic_predictions(weights,inputs)\n",
    "    label_probabilities = preds * targets + (1 - preds) * (1 - targets)\n",
    "    return -np.sum(np.log(label_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a toy dataset\n",
    "inputs = np.array([[0.52,1.12,0.77],\n",
    "                  [0.88, -1.08,0.15],\n",
    "                  [0.52,0.06,-1.30],\n",
    "                  [0.74,-2.49,1.39]])\n",
    "targets = np.array([True,True,False,True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that returns gradients of training loss using Autograd\n",
    "training_gradient_fun = grad(training_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss:  2.772588722239781\n"
     ]
    }
   ],
   "source": [
    "# Optimize weights using gradient descent\n",
    "weights = np.array([0.0,0.0,0.0])\n",
    "print(\"Initial loss: \",training_loss(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained loss:  2.772588722239781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ort/opt/anaconda3/lib/python3.7/site-packages/autograd/tracer.py:14: UserWarning: Output seems independent of input.\n",
      "  warnings.warn(\"Output seems independent of input.\")\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    weights -= training_gradient_fun(weights) * 0.01\n",
    "\n",
    "print(\"Trained loss: \",training_loss(weights))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
